{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "![Text Summarization](https://blog.fpt-software.com/hs-fs/hubfs/image-8.png?width=376&name=image-8.png)\n",
    "\n",
    "Text summarization คือ กระบวนการที่ใช้ในการสรุปข้อความยาวๆ ให้เป็นข้อความขนาดสั้นๆ ที่เข้าใจได้ง่าย และยังคงไว้ซึ่งสารที่ต้องการสื่อ\n",
    "\n",
    "โดยทั่วไป การทำสรุปจะมีสองประเภท คือ การสรุปแบบ Extractive Summary และการสรุปแบบ Abstractive Summary\n",
    "\n",
    "โดย Extractive Summary จะเป็นการเลือกประโยคที่มีใจความเด่นๆ ขึ้นมาทำเป็นสรุป\n",
    "แต่ Abstractive Summary จะเป็นการเขียนข้อความขึ้นมาใหม่ให้สั้นและกระทัดรัด"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Summary in CNN Daily Mail\n",
    "\n",
    "Term-frequency inverse-document-frequency (TF-IDF) เป็นลักษะณะเด่น หรือ feature แบบหนึ่งที่นิยมใช้กันมากในการทำ NLP\n",
    "\n",
    "โดย feature นี้จะทำการสกัดเอาความถี่ของคำ (text frequency) ที่ปรากฏในเอกสาร (document) มาใช้ โดยสมมติฐานที่ว่า \n",
    "ยิ่งความถี่ของคำสูง ยิ่งเป็นลักษณะเด่นที่สามารถใช้เป็นตัวแทน (feature) ของ doc ได้ \n",
    "\n",
    "อย่างไรก็ตาม คำบางคำเป็นเป็นคำทั่วไป ที่ปรากฏอยู่มากในทุกเอกสาร ทำให้ไม่สารถนำมาใช้เป็นตัวแทนของข้อมูลได้ดีมากนัก จึงมีการใช้\n",
    "ส่วนกลับของความถี่ของคำที่พบได้ทั่วไป (inverse document frequency) มาใช้งานร่วมด้วย\n",
    "\n",
    "**โดย TF-IDF สามารถใช้เพื่อหาคำเด่นๆ เพื่อสร้างเป็นการสรุปบทความอย่างง่ายได้**\n",
    "\n",
    "โดยในตัวอย่างนี้จะมีการแสดงขั้นตอนต่างๆ ดังนี้\n",
    "\n",
    "1. Install and setup tools\n",
    "2. Import modules\n",
    "3. Load CNN-Daily Mail data\n",
    "4. Create TF-IDF vectorizer\n",
    "5. Transform text to TF-IDF\n",
    "6. Create information for highlight visualization\n",
    "7. Visualize highlight summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Install tools \n",
    "\n",
    "[Attention Visualizer](https://github.com/abisee/attn_vis) เป็นเครื่องมือที่ใช้ในการแสดงผล highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/abisee/attn_vis 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import resource\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from IPython.display import IFrame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config open file limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Load CNN-Daily Mail data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CNN-Daily Mail data ด้วย Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tfds.load('cnn_dailymail', split='train', shuffle_files=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch ข้อมูลเข้า memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_iterator = train_dataset.take(200).as_numpy_iterator()\n",
    "\n",
    "example1 = ds_iterator.next()\n",
    "example2 = ds_iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "แสดงตัวอย่าง data ใน dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full article\n",
      "('By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . '\n",
      " 'UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic '\n",
      " 'Diocese in North Dakota has exposed potentially hundreds of church members '\n",
      " 'in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late '\n",
      " 'September and early October. The state Health Department has issued an '\n",
      " 'advisory of exposure for anyone who attended five churches and took '\n",
      " 'communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in '\n",
      " 'North Dakota has exposed potentially hundreds of church members in Fargo, '\n",
      " 'Grand Forks and Jamestown to the hepatitis A . State Immunization Program '\n",
      " \"Manager Molly Howell says the risk is low, but officials feel it's important \"\n",
      " 'to alert people to the possible exposure. The diocese announced on Monday '\n",
      " 'that Bishop John Folda is taking time off after being diagnosed with '\n",
      " 'hepatitis A. The diocese says he contracted the infection through '\n",
      " 'contaminated food while attending a conference for newly ordained bishops in '\n",
      " 'Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of '\n",
      " 'appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North '\n",
      " 'Dakota (pictured) is where the bishop is located .')\n",
      "Abstractive highlight\n",
      "('Bishop John Folda, of North Dakota, is taking time off after being diagnosed '\n",
      " '.\\n'\n",
      " 'He contracted the infection through contaminated food in Italy .\\n'\n",
      " 'Church members in Fargo, Grand Forks and Jamestown could have been exposed .')\n"
     ]
    }
   ],
   "source": [
    "print(\"Full article\")\n",
    "pprint(example1[\"article\"].decode())\n",
    "\n",
    "print(\"Abstractive highlight\")\n",
    "pprint(example1[\"highlights\"].decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Create TF-IDF vectorizer\n",
    "\n",
    "Term-frequency inverse-document-frequency (TF-IDF) เป็นลักษะณะเด่น หรือ feature แบบหนึ่งที่นิยมใช้กันมากในการทำ NLP\n",
    "\n",
    "โดย feature นี้จะทำการสกัดเอาความถี่ของคำ (text frequency) ที่ปรากฏในเอกสาร (document) มาใช้ โดยสมมติฐานที่ว่า \n",
    "ยิ่งความถี่ของคำสูง ยิ่งเป็นลักษณะเด่นที่สามารถใช้เป็นตัวแทน (feature) ของ doc ได้ \n",
    "\n",
    "อย่างไรก็ตาม คำบางคำเป็นเป็นคำทั่วไป ที่ปรากฏอยู่มากในทุกเอกสาร ทำให้ไม่สารถนำมาใช้เป็นตัวแทนของข้อมูลได้ดีมากนัก จึงมีการใช้\n",
    "ส่วนกลับของความถี่ของคำที่พบได้ทั่วไป (inverse document frequency) มาใช้งานร่วมด้วย\n",
    "\n",
    "**โดย TF-IDF สามารถใช้เพื่อหาคำเด่นๆ เพื่อสร้างเป็นการสรุปบทความอย่างง่ายได้**\n",
    "\n",
    "สำหรับตัวอย่างนี้ จะทำการสร้าง TF-IDF Vectorizer ซึ่งจะถูกใช้เพื่อแปลงบทความเป็น TF-IDF ในรูปแบบ vector 1d (ต่อบทความ)\n",
    "\n",
    "โดยจะเลือกใช้คำศัพท์ไม่เกิน 500 ค และใช้การตัดคำอย่างง่าย (แบ่งคำตามช่องว่าง)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 500  # คำ\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    stop_words='english',\n",
    "    max_features=max_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer ที่ได้จะเป็นการตัดคำแบบทั่วไป เราควรจะ fit vectorizer ให้เข้ากับ dataset ที่เราต้องการ\n",
    "\n",
    "ทำการ load ข้อมูลเข้า memory เพื่อสร้างเป็น corpus และ highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterator = train_dataset.take(200).as_numpy_iterator()\n",
    "\n",
    "corpus = [ example[\"article\"].decode() for example in training_iterator ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterator = train_dataset.take(200).as_numpy_iterator()     # reset iterator\n",
    "\n",
    "target_highlights = [ example[\"highlights\"].decode() for example in training_iterator ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทำการ fit vecorizer กับ corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=500, stop_words='english')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Transform text to TF-IDF\n",
    "\n",
    "ใช้ vectorizer ที่ได้เพื่อแปลงข้อมูลประเภท text เป็น feature  ที่มีลักษณะข้อมูลเป็น decimal vector ที่สามารถใช้เพื่อทำ machine learning ได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_features = tfidf_vectorizer.transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทำการเลือกคำที่มีลักษณะเด่นที่สุดมา 200 คำต่อบทความเพื่อสร้างเป็น summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# คำศัพท์ที่ vectorizer สกัดได้จาก corpus\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# function สำหรับการเลือกคำศัพท์ที่เด่นที่สุดมา top_n คำ\n",
    "def top_words(response, top_n=200):\n",
    "    sorted_indices = np.argsort(response[0, :])[0, :-(top_n+1):-1]\n",
    "    return feature_names[sorted_indices].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_200_word_summaries = [ \" \".join(top_words(article_feature)) for article_feature in article_features ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ลองแสดงผลลัพท์การสรุปข้อความที่ได้จากวิธีการที่ใช้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'october north members hundreds john 25 2013 est state pictured says manager food press officials announced health september possible late department monday 15 feel 11 14 early taking published updated took month people time france following general forced gave friday forces friends foreign free game friend future forward games young flight europe discovered does doesn dog doing don dr drug earlier east end evidence final experience face fact family fans getting far father federal felt fight germany ground given iraq human idea including india information injury inside instead international interview investigation involved island house james january job johnson joy judge june just killed king know known huge hours goal having going gone good got government great died group half hand happened hard head hospital heard hearing held help helped high history hit hiv holiday home hope different deal didn britain appeared area areas asked assault atletico attack attacks attorney australia authorities away baby ball bank based began believe believed best better big birth black board body border amish american america 28 10 100 12 13 16 17 18 20 2008 2010 2012 22 30 allowed able according account accused action added admitted age ago ahead air al bring british did'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_200_word_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "คำนวณ rouge metrics ที่ใช้วัดผลการทำสรุปบทความ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:\n",
      "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
      "He contracted the infection through contaminated food in Italy .\n",
      "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
      "Prediction:\n",
      "october north members hundreds john 25 2013 est state pictured says manager food press officials announced health september possible late department monday 15 feel 11 14 early taking published updated took month people time france following general forced gave friday forces friends foreign free game friend future forward games young flight europe discovered does doesn dog doing don dr drug earlier east end evidence final experience face fact family fans getting far father federal felt fight germany ground given iraq human idea including india information injury inside instead international interview investigation involved island house james january job johnson joy judge june just killed king know known huge hours goal having going gone good got government great died group half hand happened hard head hospital heard hearing held help helped high history hit hiv holiday home hope different deal didn britain appeared area areas asked assault atletico attack attacks attorney australia authorities away baby ball bank based began believe believed best better big birth black board body border amish american america 28 10 100 12 13 16 17 18 20 2008 2010 2012 22 30 allowed able according account accused action added admitted age ago ahead air al bring british did\n",
      "ROUGE-1 '0.0341880317042883'\n",
      "ROUGE-2 '0.0'\n",
      "ROUGE-L '0.03433476151706625'\n",
      "ROUGE-AVG '0.022840931073784848' \n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try evaluate rouge metric of first example\n",
    "hypo = top_200_word_summaries[0]\n",
    "target = target_highlights[0]\n",
    "\n",
    "rouge = Rouge()\n",
    "rouges = rouge.get_scores(hypo, target)[0]\n",
    "r1_val, r2_val, rl_val = rouges['rouge-1'][\"f\"], rouges['rouge-2'][\"f\"], rouges['rouge-l'][\"f\"]\n",
    "\n",
    "print('Target:')\n",
    "print(target)\n",
    "print('Prediction:')\n",
    "print(hypo)\n",
    "\n",
    "print(f\"ROUGE-1 '{r1_val}'\")\n",
    "print(f\"ROUGE-2 '{r2_val}'\")\n",
    "print(f\"ROUGE-L '{rl_val}'\")\n",
    "print(f\"ROUGE-AVG '{np.mean([r1_val, r2_val, rl_val])}'\", '\\n--\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Create information for highlight visualization\n",
    "\n",
    "เครื่องมือที่จำใช้ visualize คือ\n",
    "\n",
    "ซึ่งต้องการ format ข้อมูลสำหรับการแสดงผลดังนี้\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"article_lst\": article_lst,     # 1d list N_ariticle_tokens\n",
    "    \"p_gens\": prob_vocab,           # 2d array N_decoded_lst x 1\n",
    "    \"decoded_lst\": decoded_lst,     # 1d list N_hypo_tokens\n",
    "    \"abstract_str\": target,         # a string\n",
    "    \"attn_dists\": dists             # Equivalence to emission matrix in HMM   # N_decoded_lst * tf_idf  \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สร้างฟั่งชั่นที่ใช้ gen tokens (list of words) และ attention distribution จาก tf-idf ด้วยความเข้าใจ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_used_word = np.sum(list(tfidf_vectorizer.vocabulary_.values()))\n",
    "\n",
    "# extract sorted text tokens and attention distribution from tf-idf feature\n",
    "def get_viz_info_tf_idf(article, article_feature, top_n=200):\n",
    "    sorted_indices = np.argsort(article_feature[0, :])[0, :-(top_n+1):-1]\n",
    "    \n",
    "    # text tokens aka decoded list\n",
    "    decoded_lst = feature_names[sorted_indices].tolist()[0]\n",
    "    \n",
    "    # prob distribution for\n",
    "    dists = tfidf_vectorizer.transform(decoded_lst).todense().tolist()\n",
    "\n",
    "    # prob from vocabulary\n",
    "    p_gens = article_feature[0, sorted_indices].transpose().tolist()\n",
    "    \n",
    "    return p_gens, decoded_lst, np.vstack(dists).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format ข้อมูลที่ใช้แสดงผลและบันทึกเป็นไฟล์ไว้ให้ [Attention Visualizer](https://github.com/abisee/attn_vis) แสดงผล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_lst = corpus[0].split(\" \")  # simple tokenize\n",
    "p_gens, decoded_lst, dists = get_viz_info_tf_idf(article_lst, article_features[0], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_viz_json = {\n",
    "    \"article_lst\": article_lst,     # 1d list N_ariticle_tokens\n",
    "    \"p_gens\": p_gens,      # 2d array N_decoded_lst x 1\n",
    "    \"decoded_lst\": decoded_lst,     # 1d list N_hypo_tokens\n",
    "    \"abstract_str\": target,         # a string\n",
    "    \"attn_dists\": dists             # Equivalence to emission matrix in HMM   # N_decoded_lst * tf_idf  \n",
    "}\n",
    "with open('./attn_vis/attn_vis_data.json', 'w') as outfile:\n",
    "    json.dump(att_viz_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Visualize highlight summary\n",
    "\n",
    "run shell command ใน project directory\n",
    "```shell\n",
    "cd attn_vis\n",
    "python3 -m http.server\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8000\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdd284d29b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src='http://localhost:8000', width=900, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
